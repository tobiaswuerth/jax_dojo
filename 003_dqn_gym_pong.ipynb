{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "import tqdm\n",
    "from collections import deque\n",
    "from typing import NamedTuple\n",
    "import numpy as np\n",
    "import random as py_random\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, grad, random\n",
    "\n",
    "import optax\n",
    "\n",
    "import flax.linen as nn\n",
    "from flax.training.train_state import TrainState\n",
    "\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "from gymnasium.wrappers.atari_preprocessing import AtariPreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    action_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Conv(features=32, kernel_size=(8, 8), strides=(4, 4))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features=64, kernel_size=(4, 4), strides=(2, 2))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features=64, kernel_size=(3, 3), strides=(1, 1))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        x = nn.Dense(features=512)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.action_dim)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(NamedTuple):\n",
    "    observation: np.ndarray\n",
    "    observation_next: np.ndarray\n",
    "    action: int\n",
    "    reward: float\n",
    "    done: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.9.0+750d7f9)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "max_episode_steps=1000\n",
    "\n",
    "def create_env():\n",
    "    env: gym.Env = gym.make(\n",
    "        id=\"ALE/Pong-v5\",\n",
    "        max_episode_steps=max_episode_steps,\n",
    "        autoreset=False\n",
    "    )\n",
    "    return AtariPreprocessing(\n",
    "        env,\n",
    "        noop_max=30,\n",
    "        frame_skip=1,  # Pong-v5 has frame_skip=4 by default\n",
    "        screen_size=84,\n",
    "        terminal_on_life_loss=False,\n",
    "        grayscale_obs=True,\n",
    "        grayscale_newaxis=True,\n",
    "        scale_obs=False,\n",
    "    )\n",
    "\n",
    "env = create_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(env.action_space.n)\n",
    "rng = random.key(0)\n",
    "variables = model.init(rng, jnp.zeros((1, *env.observation_space.shape)))\n",
    "\n",
    "tx = optax.adam(1e-4)\n",
    "state = TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=variables[\"params\"],\n",
    "    tx=tx,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(params, batch:Transition):\n",
    "    x = jnp.array(batch.observation).astype(jnp.float32) / 255.0\n",
    "    x_next = jnp.array(batch.observation_next).astype(jnp.float32) / 255.0\n",
    "    rewards = jnp.array(batch.reward)\n",
    "    dones = jnp.array(batch.done)\n",
    "    actions = jnp.array(batch.action)\n",
    "    \n",
    "    state_action_values = state.apply_fn({\"params\": state.params}, x)\n",
    "    next_state_values = state.apply_fn({\"params\": state.params}, x_next)\n",
    "    next_state_values = jnp.max(next_state_values, axis=1)\n",
    "    next_state_values = jnp.where(dones, jnp.zeros_like(next_state_values), next_state_values)\n",
    "    \n",
    "    expected = rewards + 0.99 * next_state_values\n",
    "    actual = state_action_values[jnp.arange(x.shape[0]), actions]\n",
    "    loss = jnp.mean((expected - actual) ** 2)\n",
    "    return loss\n",
    "\n",
    "@jit\n",
    "def update(state:TrainState, batch):\n",
    "    loss, grads = jax.value_and_grad(loss_fn, allow_int=True)(state.params, batch)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1\n",
    "epsilon_decay = 0.95\n",
    "\n",
    "def get_action(observation):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "\n",
    "    observation = jnp.array(observation).astype(jnp.float32) / 255.0\n",
    "    q_values = state.apply_fn({\"params\": state.params}, observation[None, ...])\n",
    "    return jnp.argmax(q_values).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 947/1000 [00:01<00:00, 729.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 rewards: -20.0, losses: 0.03066929802298546, epsilon: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 950/1000 [00:16<00:00, 56.69it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 rewards: -20.0, losses: 0.030415983870625496, epsilon: 0.9025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 905/1000 [00:18<00:01, 47.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2 rewards: -20.0, losses: 0.028568534180521965, epsilon: 0.8573749999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 934/1000 [00:38<00:02, 24.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3 rewards: -21.0, losses: 0.028507616370916367, epsilon: 0.8145062499999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 828/1000 [00:34<00:07, 24.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4 rewards: -21.0, losses: 0.02430008165538311, epsilon: 0.7737809374999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 952/1000 [00:45<00:02, 21.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5 rewards: -20.0, losses: 0.02806958183646202, epsilon: 0.7350918906249998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 812/1000 [00:50<00:11, 16.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6 rewards: -21.0, losses: 0.03618348762392998, epsilon: 0.6983372960937497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 976/1000 [00:59<00:01, 16.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7 rewards: -19.0, losses: 0.02941839210689068, epsilon: 0.6634204312890623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 918/1000 [02:02<00:10,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8 rewards: -21.0, losses: 0.03079039230942726, epsilon: 0.6302494097246091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 871/1000 [02:05<00:18,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9 rewards: -21.0, losses: 0.028599319979548454, epsilon: 0.5987369392383786\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "batch_size = 32\n",
    "\n",
    "memory = []\n",
    "memory_size = 10_000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    rewards = 0\n",
    "    losses = []\n",
    "    for i in tqdm.trange(max_episode_steps):\n",
    "        action = get_action(observation)\n",
    "        observation_next, reward, done, truncated, info = env.step(action)\n",
    "        done = done or truncated\n",
    "        transition = Transition(observation, observation_next, action, reward, done)\n",
    "        \n",
    "        memory.append(transition)\n",
    "        if len(memory) > memory_size:\n",
    "            memory.pop(0)\n",
    "        \n",
    "        rewards += reward\n",
    "        observation = observation_next\n",
    "\n",
    "        if len(memory) >= batch_size and i % 16 == 0:\n",
    "            batch = py_random.sample(memory, batch_size)\n",
    "            batch = Transition(*zip(*batch))\n",
    "            state, loss = update(state, batch)\n",
    "            losses.append(loss)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    epsilon *= epsilon_decay\n",
    "    print(f\"Episode {episode} rewards: {rewards}, losses: {np.mean(losses)}, epsilon: {epsilon}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
